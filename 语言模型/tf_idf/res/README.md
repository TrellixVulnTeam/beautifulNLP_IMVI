# 训练TF-IDF文档向量，用于信息检索

### 项目说明

> 此项目是英文信息检索系统的一部分，我主要的工作是训练TF-IDF文档向量，主要是用于练习。

### 训练数据

> 从国外新闻网站爬取的数据。然后处理成如下规范格式：
>
> - 第一行为网站网址 (http...)
> - 第二行为文章标题
> - 以下多行为文章正文
>
> 可以通过**http开头**区分不同的文章

### 模型介绍

> 模型的整体步骤如下：
>
> 1. 数据清洗：来自网页的文本数据包含很多特殊符号，在启动模型前需要进行清洗。
>
> 2. 构造停用词表：停用词在很多语言中很常见，我们在网上下载停用表。
>
> 3. 词干提取（stemming）：我们选择一个40年之久的词干提取算法，用于移除单词后缀。将去除停用词的文本词干提取，得到词干提取后的原始文本。
>
> 4. 构造一个词表（term_list）：通过词干提取后的文本，构造一个词表，这个词表是IR系统的基础资源。
>
> 5. 统计词频（TF）：统计每个文档的词频。然而，为了提高系统的性能，标题和第一段的单词数量乘以X和Y，X和Y的初始值设置为2，可以通过调整这两个值，调整模型。**每个文档对应的单词都有一个在该文档的词频。**
>
> 6. 逆文档频率（IDF）：逆文档频率是为了表示单词提供了多少信息。通常的计算是log(N/n)，其中N是语料中的文档总数，n是出现当前词的文档数。**每个词有一个IDF**。
>
> 7. 词的权重计算：计算词的权重（w），w = tf * idf。每个词有一个权重。
>
> 8. 向量标准化：完成上面步骤后，每个文档被表示为一个向量**v**。**v**的标准化是，**v** = **v** / |**v**|。这步是可选的，那将容易计算cos(**v1**, **v2**)为**v1**和 **v2**的点乘。
>     $$
>     cos(v1, v2) = \frac{v1 \cdot  v2}{|v1| \cdot |v2|}
>     $$
>
>     $$
>     if \quad v = v / |v| \quad then \quad cos(v1, v2) = v1 \cdot  v2
>     $$
>
> 最终源文件、地址链接文件、标题文件、内容文件一一对应。词表文件（term_list）作为信息检索的核心文件。得到的TF-IDF向量和源文件一一对应，大约4.9G左右。
>
> porter_stemmer.py为官网下载的代码，用于词干提取。
