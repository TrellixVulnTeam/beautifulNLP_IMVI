# 使用gensim训练word2vec词向量

### 数据预处理

>去除停用词和标点，将文本处理成每行一句话，每句话中词用空格隔开。格式如下：
>
>```
>数 学 利 用 符 号 语 言 研 究 数 量 结 构 变 化 空 间 概 念 一 门 学 科
>某 种 角 度 看 属 于 形 式 科 学 一 种 数 学 透 过 抽 象 化 逻 辑 推 理 使 用 计 数 
>计 算 量 度 物 体 形 状 运 动 观 察 产 生 数 学 家 
>拓 展 概 念 公 式 化 新 猜 想 选 定 公 理 
>```

### 参数说明

> 1. sentences: 我们分析的语料，可以是一个**列表**，或者从**文件**中遍历读取。
> 2. size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料库的大小相关，如果语料库不大，比如小于100M的语料，则默认值就可以了。如果是超大语料，建议增大维度。
> 3. window：即词向量上下文最大距离，window越大，则和某一个词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际需求来动态调整这个window大小。如果是小语料，则这个词可以设置更小，对于一般语料这个值可以推荐在**[5, 10]**之间。
> 4. sg: 即是word2vec两个模型的选择了。如果是0，则是CBOW模型，是1，则是Skip-gram模型。默认是0，CBOW模型。
> 5. hs: 即是我们的word2vec两个模型的解法的选择了，如果是0，则是Negative Sampling；是1的话并且负采样个数negtive**大于**0，则是Hierarchical Softmax。默认是0即Negative Sampling。
> 6. negtive: 即使用Negative Samping时负采样的个数，默认是5。推荐在[3, 10]之间。
> 7. cbow_mean: 仅用于CBOW在做投影时使用，为0，则算法中为上下文的词向量之和；为1，则为上下文词向量的平均值。个人比较喜欢使用平均值来表示，默认值也是为1，不推荐修改默认值。
> 8. min_count: 需要计算词向量的最小词频。这个值可以去掉一些很**生僻**的低频词，默认是5。如果是小语料，可以调低这个值。
> 9. iter: 随机梯度下降中迭代的最大次数，默认值是5。对于大语料可以增大这个值。
> 10. alpha: 在随机梯度下降算法中迭代的初始步长，默认值是0.025。
> 11. min_alpha: 由于算法支持在迭代过程中逐步减小步长，min_alpha给出了最小步长值。随机梯度下降中每轮的迭代步长可以由iter, alpha, min_alpha一起得出。**对于大语料，需要对alpha，min_alpha，iter一起调参，来选择合适的三个值**。
> 12. LineSentence： 从文件中逐行读取。
> 13. workers=multiprocessing.cpu_count()：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。

### 调参技巧

> 1. 选择的训练word2vec的语料要和要使用词向量的任务相似，并且越大越好，论文中实验说明语料比训练词向量的模型更加的重要，所以要尽量收集大的且与任务相关的语料来训练词向量；
> 2. 语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型；
> 3. 设置迭代次数为三五十次，维度至少选 50，常见的词向量的维度为256、512以及处理非常大的词表的时候的1024维；

### 存储与加载

> 使用joblib模块存储与加载

### 数据说明

> 将下载的数据放在`data`目录下，下载地址在主页的百度网盘链接中。